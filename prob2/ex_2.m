fprintf('Exercise 2: Test radial basis kernel regression.\n');
fprintf('By Michelli theorem, the Gram matrix of distinct points generated by radial basis kernel is always invertible.\n');
fprintf('Show the effect of lambda\n');
% Load in data for problem 1-a
load_data
[n,d]=size(X);
cut_idx = ceil(n/2);
train_idx = 1:cut_idx;
test_idx = cut_idx+1:n;
% X_train = X(train_idx,:);
% X_test = X(test_idx,:);

% Regularization parameter
lambda = [0.03 0.1 0.15 0.1950 0.3 1 3];
exp_times = length(lambda);

% Errors
train_error = zeros(1,exp_times);
test_error = zeros(1,exp_times);
true_error = zeros(1,exp_times);

% Construct rbf kernel
% t=20;
% Parameter in rbf kernel (Optimized)
t=505;
this_rbf=@(X,Y)(rbf_kernel(X,Y,t));

for i=1:exp_times
    % Kernel regression
    alpha = kernel_regress(X(train_idx,:),y_noisy(train_idx),this_rbf,lambda(i));
    % Get train error
    y_train = kernel_pred(X(train_idx,:),X(train_idx,:),alpha,this_rbf);
    train_error(i)=norm(y_train-y_noisy(train_idx))/length(train_idx);
    % Get test error
    y_test = kernel_pred(X(test_idx,:),X(train_idx,:),alpha,this_rbf);
    test_error(i) = norm(y_test-y_noisy(test_idx))/length(test_idx);
    % Get true error
    y_temp = kernel_pred(X,X(train_idx,:),alpha,this_rbf);
    true_error(i) = norm(y_temp-y_true)/length(y_true);
    if mod(i,2)==0
        % Plot prediction surface and y_noisy(test_idx)
        ax = plot_pred_surf([min(X(:,1)),max(X(:,1));min(X(:,2)),max(X(:,2))],[100 100],X(train_idx,:),alpha,this_rbf);
        ax.ZLim=[min(y_noisy),max(y_noisy)];
        scatter3(ax,X(:,1),X(:,2),y_noisy);
        title(ax,sprintf('Prediction Surface & Data With \\lambda = %.2f',lambda(i)));
    end
end

figure
p=plot(lambda,train_error,'LineWidth',2,'Marker','x');
ax = p.Parent;
hold on
plot(lambda,test_error,'LineWidth',2,'Marker','o');
% plot(lambda,true_error,'LineWidth',2,'Marker','*');
legend('Train','Test');
title('Mean Squared Error (MSE)');
xlabel('\lambda');
ylabel('MSE');
ax.XScale='log';

fprintf('Conclusion:\n');
fprintf('We will discuss the effects of several hyper-parameters\n');
fprintf('In this file we will discover the effect of lambda; In show_t_effect.m we will discuss the effect of t in radial basis kernel.\n'); 
fprintf('First let''s see what if lambda goes to 0. If lambda = 0, then there will be no regularization, and as can be shown, the resulting training error will be exactly 0, and it is unlikely for the model to generalize well.\n');
fprintf('So if lambda is too small, the resultant model will tend to be over-fit\n');
fprintf('On contrary, what happens if lambda goes to +inf? This will reduce the height of prediction, making the prediction surface flatter, thus the resulting model of cause will be under-fit.\n');
fprintf('Therefore, an appropriate lambda should be chosen around 1.\n');
